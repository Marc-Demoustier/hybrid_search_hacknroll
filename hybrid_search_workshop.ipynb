{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search & Smart Flashcard Generator\n",
    "\n",
    "In this workshop, you'll learn how modern search engines combine **lexical search** (keyword matching) with **semantic search** (meaning-based) to deliver better results. Then you'll build a **Smart Flashcard Generator** that uses this technology!\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How BM25 (lexical search) works - the algorithm behind Elasticsearch\n",
    "2. How semantic search uses neural embeddings to understand meaning\n",
    "3. Why neither approach alone is perfect\n",
    "4. How to combine them with **hybrid search**\n",
    "5. **RAG (Retrieval-Augmented Generation)** - combining search with LLMs\n",
    "6. Build a working **flashcard generator** you can use for studying!\n",
    "\n",
    "**The Pipeline:**\n",
    "```\n",
    "Your Notes â†’ Hybrid Search (retrieve relevant content) â†’ LLM (generate Q&A) â†’ Flashcards (Anki export!)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Pre-workshop Setup (Do This Before the Workshop!)\n",
    "\n",
    "To make the most of our limited time, please run these setup steps **before** the workshop. This downloads the required models (~9GB total) which can take 10-15 minutes.\n",
    "\n",
    "### Step 1: Install packages (~2 min)\n",
    "Run the cell below to install all required packages.\n",
    "\n",
    "### Step 2: Download models (~10-15 min)\n",
    "Run the second cell to pre-download the AI models. This only needs to be done once - the models are cached for future use.\n",
    "\n",
    "### Step 3: Verify setup\n",
    "If both cells complete without errors, you're ready for the workshop! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install packages\n",
    "!pip install requests beautifulsoup4 rank_bm25 sentence-transformers faiss-cpu networkx pyvis gradio tqdm numpy transformers accelerate bitsandbytes genanki pypdf -q\n",
    "print(\"âœ… Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Pre-download models (run this before the workshop!)\n",
    "print(\"Downloading models... This takes 10-15 minutes on first run.\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Download sentence-transformers model (~90MB)\n",
    "print(\"\\n1/2: Downloading embedding model...\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Embedding model ready!\")\n",
    "\n",
    "# Download Qwen3-8B (~8GB)\n",
    "print(\"\\n2/2: Downloading Qwen3-8B (this is the big one, ~8GB)...\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# Detect device and download with appropriate settings\n",
    "if torch.cuda.is_available():\n",
    "    print(\"   CUDA detected - downloading for GPU...\")\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"   Apple Silicon detected - downloading for MPS...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "else:\n",
    "    print(\"   CPU mode - downloading (will be slow during workshop)...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", device_map=\"auto\")\n",
    "\n",
    "print(\"âœ… Qwen3-8B ready!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸŽ‰ All set! You're ready for the workshop!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Clean up memory\n",
    "del model, tokenizer, embed_model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Search Problem\n",
    "\n",
    "Why is finding relevant content so hard?\n",
    "\n",
    "**Scenario 1:** You search for \"Singapore economy\"\n",
    "- A great article titled \"GDP Growth in the Lion City\" exists\n",
    "- But it doesn't contain the exact words \"Singapore\" or \"economy\"!\n",
    "- Traditional keyword search would miss it completely\n",
    "\n",
    "**Scenario 2:** You search for \"BM25 algorithm\"\n",
    "- You want technical documentation with that exact term\n",
    "- A semantic search might return general \"search algorithm\" articles\n",
    "- Exact keyword matching would be more helpful here\n",
    "\n",
    "**The solution?** Combine both approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|---------|\n",
    "| `requests` | Make HTTP requests to fetch web pages |\n",
    "| `beautifulsoup4` | Parse HTML and extract content/links |\n",
    "| `rank_bm25` | BM25 algorithm implementation |\n",
    "| `sentence-transformers` | Generate text embeddings for semantic search |\n",
    "| `faiss-cpu` | Fast similarity search in vector space |\n",
    "| `networkx` | Build and analyze link graphs |\n",
    "| `pyvis` | Interactive graph visualizations (double-click to open URLs!) |\n",
    "| `gradio` | Build web interfaces for our app |\n",
    "| `tqdm` | Show progress bars during crawling |\n",
    "| `numpy` | Numerical operations for embeddings |\n",
    "| `transformers` | HuggingFace library for loading LLMs |\n",
    "| `accelerate` | Efficient model loading across devices |\n",
    "| `bitsandbytes` | 4-bit quantization (reduces memory 4x) |\n",
    "| `genanki` | Create Anki flashcard decks (.apkg) |\n",
    "| `pypdf` | Extract text from PDF files |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests beautifulsoup4 rank_bm25 sentence-transformers transformers accelerate bitsandbytes faiss-cpu networkx pyvis gradio tqdm numpy genanki pypdf -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Our Document Collection\n",
    "\n",
    "Before we can search, we need documents to search through! We'll:\n",
    "1. Crawl Wikipedia pages to build a corpus\n",
    "2. Allow adding custom documents\n",
    "\n",
    "### What is a Web Crawler?\n",
    "\n",
    "A **web crawler** (also called a spider or bot) is a program that automatically browses the web to collect information. Here's how it works:\n",
    "\n",
    "1. **Start with a seed URL** - Give it a starting webpage (e.g., a Wikipedia article)\n",
    "2. **Download the page** - Fetch the HTML content\n",
    "3. **Extract links** - Find all links to other pages\n",
    "4. **Follow the links** - Visit those pages and repeat\n",
    "5. **Store the content** - Save the text for later use (like searching!)\n",
    "\n",
    "```\n",
    "ðŸŒ Start URL â†’ ðŸ“„ Download Page â†’ ðŸ”— Find Links â†’ ðŸ”„ Repeat\n",
    "                      â†“\n",
    "                 ðŸ’¾ Store Content\n",
    "```\n",
    "\n",
    "**Why do search engines need crawlers?** Search engines like Google can't search the entire internet in real-time. Instead, they use crawlers to discover and download billions of pages *in advance*, storing them in a massive index. When you search, you're actually searching this pre-built index, not the live web!\n",
    "\n",
    "In this workshop, we'll build a simple crawler to collect Wikipedia articles for our search experiments.\n",
    "\n",
    "### Ethical Web Crawling\n",
    "\n",
    "When crawling websites, we must be respectful:\n",
    "- **Respect `robots.txt`** - Check what's allowed to crawl\n",
    "- **Identify yourself** - Use a proper User-Agent header\n",
    "- **Rate limiting** - Don't overwhelm servers\n",
    "\n",
    "### What is robots.txt?\n",
    "\n",
    "Every website can have a `robots.txt` file at their root that tells crawlers what they're allowed to access. Let's fetch some real examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "USER_AGENT = \"HybridSearchWorkshop/1.0 (Educational)\"\n",
    "REQUEST_DELAY = 0.5  # seconds between requests\n",
    "\n",
    "class WebCrawler:\n",
    "    \"\"\"\n",
    "    General-purpose web crawler that respects robots.txt.\n",
    "    \n",
    "    This base class can crawl any website. Subclass it to add \n",
    "    site-specific logic (like WikipediaCrawler below).\n",
    "    \n",
    "    Usage:\n",
    "        crawler = WebCrawler()\n",
    "        data = crawler.crawl(\"https://example.com\", max_pages=10)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_agent: str = USER_AGENT, delay: float = REQUEST_DELAY) -> None:\n",
    "        self.user_agent = user_agent\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers[\"User-Agent\"] = user_agent\n",
    "        self.robots_cache = {}  # Cache robots parsers per domain\n",
    "    \n",
    "    def get_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract the domain from a URL.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "    \n",
    "    def can_fetch(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is allowed by robots.txt.\"\"\"\n",
    "        domain = self.get_domain(url)\n",
    "        if domain not in self.robots_cache:\n",
    "            try:\n",
    "                rp = RobotFileParser()\n",
    "                robots_url = f\"{domain}/robots.txt\"\n",
    "                robots_txt = self.session.get(robots_url, timeout=5).text\n",
    "                rp.parse(robots_txt.splitlines())\n",
    "                self.robots_cache[domain] = rp\n",
    "            except:\n",
    "                # If we can't fetch robots.txt, assume allowed\n",
    "                return True\n",
    "        return self.robots_cache[domain].can_fetch(\"*\", url)\n",
    "    \n",
    "    def get(self, url: str, **kwargs) -> requests.Response:\n",
    "        \"\"\"Make a GET request with proper headers.\"\"\"\n",
    "        kwargs.setdefault('timeout', 10)\n",
    "        return self.session.get(url, **kwargs)\n",
    "    \n",
    "    def get_title(self, url: str, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract page title. Override in subclass for custom logic.\"\"\"\n",
    "        title_tag = soup.find('title')\n",
    "        return title_tag.get_text(strip=True) if title_tag else url\n",
    "    \n",
    "    def extract_text(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract main text content. Override in subclass for custom logic.\"\"\"\n",
    "        # Remove script and style elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            tag.decompose()\n",
    "        return soup.get_text(separator=' ', strip=True)[:5000]\n",
    "    \n",
    "    def is_valid_link(self, href: str, base_url: str) -> bool:\n",
    "        \"\"\"Check if a link should be followed. Override in subclass.\"\"\"\n",
    "        if not href:\n",
    "            return False\n",
    "        # Skip anchors, javascript, mailto, etc.\n",
    "        if href.startswith(('#', 'javascript:', 'mailto:', 'tel:')):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def extract_links(self, soup: BeautifulSoup, base_url: str) -> list[str]:\n",
    "        \"\"\"Extract all valid links from the page.\"\"\"\n",
    "        links = []\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if self.is_valid_link(href, base_url):\n",
    "                # Convert relative URLs to absolute\n",
    "                full_url = urljoin(base_url, href)\n",
    "                # Only follow links on the same domain\n",
    "                if self.get_domain(full_url) == self.get_domain(base_url):\n",
    "                    links.append(full_url)\n",
    "        return list(dict.fromkeys(links))  # Remove duplicates, preserve order\n",
    "    \n",
    "    def crawl_page(self, url: str) -> dict | None:\n",
    "        \"\"\"Crawl a single page and extract title, text, and links.\"\"\"\n",
    "        if not self.can_fetch(url):\n",
    "            print(f\"[robots.txt blocked] {url}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            response = self.get(url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"[HTTP {response.status_code}] {url}\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': self.get_title(url, soup),\n",
    "                'text': self.extract_text(soup),\n",
    "                'links': self.extract_links(soup, url)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl(self, seed_url: str, max_pages: int = 20) -> dict[str, dict]:\n",
    "        \"\"\"\n",
    "        Crawl starting from seed_url using Breadth-First Search (BFS).\n",
    "        \n",
    "        Args:\n",
    "            seed_url: Starting URL\n",
    "            max_pages: Maximum number of pages to crawl\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping URL -> page data\n",
    "        \"\"\"\n",
    "        crawled = {}\n",
    "        visited = set()\n",
    "        queue = deque([seed_url])\n",
    "        \n",
    "        pbar = tqdm(total=max_pages, desc=\"Crawling\")\n",
    "        \n",
    "        while queue and len(crawled) < max_pages:\n",
    "            url = queue.popleft()\n",
    "            if url in visited:\n",
    "                continue\n",
    "            \n",
    "            visited.add(url)\n",
    "            result = self.crawl_page(url)\n",
    "            \n",
    "            if result:\n",
    "                crawled[url] = result\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Crawling: {result['title'][:25]}...\")\n",
    "                \n",
    "                # Add new links to the queue (BFS)\n",
    "                for link in result['links']:\n",
    "                    if link not in visited:\n",
    "                        queue.append(link)\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"\\nCrawled {len(crawled)} pages!\")\n",
    "        return crawled\n",
    "\n",
    "    def show_robots_txt(self, url: str, max_rules: int = 10) -> None:\n",
    "        \"\"\"Fetch and display important rules from a site's robots.txt.\"\"\"\n",
    "        try:\n",
    "            response = self.get(f\"{self.get_domain(url)}/robots.txt\")\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ðŸ“„ {self.get_domain(url)}/robots.txt\")\n",
    "            print('='*50)\n",
    "            \n",
    "            # Filter to show only important lines\n",
    "            important_prefixes = ('user-agent:', 'disallow:', 'allow:', 'sitemap:')\n",
    "            rules_shown = 0\n",
    "            \n",
    "            for line in response.text.split('\\n'):\n",
    "                line_lower = line.lower().strip()\n",
    "                if any(line_lower.startswith(prefix) for prefix in important_prefixes):\n",
    "                    print(line.strip())\n",
    "                    rules_shown += 1\n",
    "                    if rules_shown >= max_rules:\n",
    "                        print(\"...\")\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "print(\"WebCrawler base class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at real robots.txt files using our crawler!\n",
    "crawler = WebCrawler()\n",
    "\n",
    "# Wikipedia - generally permissive\n",
    "crawler.show_robots_txt(\"https://en.wikipedia.org\", max_rules=8)\n",
    "\n",
    "# Google - much more restrictive\n",
    "crawler.show_robots_txt(\"https://www.google.com\", max_rules=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test our crawler's can_fetch() method\n",
    "test_crawler = WebCrawler()\n",
    "\n",
    "print(\"Testing if our crawler can access these URLs:\\n\")\n",
    "\n",
    "# Wikipedia - should be allowed\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/Search_engine\"\n",
    "print(f\"Wikipedia article: {test_crawler.can_fetch(wiki_url)}\")\n",
    "\n",
    "# Google search - should be blocked for generic bots\n",
    "google_url = \"https://www.google.com/search?q=test\"\n",
    "print(f\"Google search page: {test_crawler.can_fetch(google_url)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaCrawler(WebCrawler):\n",
    "    \"\"\"\n",
    "    Wikipedia-specific crawler that extends WebCrawler.\n",
    "    \n",
    "    Customizations:\n",
    "    - Extracts clean titles from Wikipedia URLs\n",
    "    - Only follows links to Wikipedia articles (skips Special:, File:, etc.)\n",
    "    - Extracts text from the main content area only\n",
    "    \n",
    "    Usage:\n",
    "        crawler = WikipediaCrawler()\n",
    "        data = crawler.crawl(\"https://en.wikipedia.org/wiki/Python\", max_pages=20)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Wikipedia namespaces to skip\n",
    "    SKIP_NAMESPACES = [\n",
    "        'File:', 'Category:', 'Help:', 'Template:', 'Wikipedia:', \n",
    "        'Special:', 'Talk:', 'Portal:', 'Module:', 'Draft:', 'MediaWiki:'\n",
    "    ]\n",
    "    \n",
    "    def get_title(self, url: str, soup: BeautifulSoup | None = None) -> str:\n",
    "        \"\"\"Extract a clean title from a Wikipedia URL.\"\"\"\n",
    "        path = urlparse(url).path\n",
    "        title = path.split('/')[-1]\n",
    "        return unquote(title).replace('_', ' ')\n",
    "    \n",
    "    def is_valid_link(self, href: str, base_url: str) -> bool:\n",
    "        \"\"\"Only follow links to Wikipedia articles.\"\"\"\n",
    "        if not href:\n",
    "            return False\n",
    "        if not href.startswith('/wiki/'):\n",
    "            return False\n",
    "        # Skip special namespaces\n",
    "        for ns in self.SKIP_NAMESPACES:\n",
    "            if href.startswith(f'/wiki/{ns}'):\n",
    "                return False\n",
    "        # Skip anchor links\n",
    "        if '#' in href:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def extract_text(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract text from Wikipedia's main content area.\"\"\"\n",
    "        content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove non-content elements\n",
    "        for tag in content_div(['script', 'style', 'sup', 'table', 'nav']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        return content_div.get_text(separator=' ', strip=True)[:5000]\n",
    "    \n",
    "    def extract_links(self, soup: BeautifulSoup, base_url: str) -> list[str]:\n",
    "        \"\"\"Extract links from Wikipedia's main content area.\"\"\"\n",
    "        content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "        if not content_div:\n",
    "            return []\n",
    "        \n",
    "        links = []\n",
    "        for a in content_div.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if self.is_valid_link(href, base_url):\n",
    "                full_url = urljoin('https://en.wikipedia.org', href)\n",
    "                if full_url != base_url:\n",
    "                    links.append(full_url)\n",
    "        \n",
    "        return list(dict.fromkeys(links))\n",
    "\n",
    "# Create crawler instances\n",
    "crawler = WikipediaCrawler()  # For Wikipedia (used in this workshop)\n",
    "\n",
    "print(\"WikipediaCrawler ready!\")\n",
    "print(\"\\nYou can also use WebCrawler for other websites:\")\n",
    "print(\"  generic_crawler = WebCrawler()\")\n",
    "print(\"  data = generic_crawler.crawl('https://any-website.com', max_pages=10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The crawl() method is now part of the WebCrawler class\n",
    "# These helper functions are kept for backwards compatibility\n",
    "\n",
    "def crawl_wikipedia_page(url: str) -> dict | None:\n",
    "    \"\"\"Crawl a single Wikipedia page. (Uses WikipediaCrawler internally)\"\"\"\n",
    "    return crawler.crawl_page(url)\n",
    "\n",
    "def crawl_wikipedia(seed_url: str, max_pages: int = 30) -> dict[str, dict]:\n",
    "    \"\"\"Crawl Wikipedia starting from seed_url. (Uses WikipediaCrawler internally)\"\"\"\n",
    "    return crawler.crawl(seed_url, max_pages=max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How BFS Works\n",
    "\n",
    "Our crawler uses **BFS (Breadth-First Search)** - exploring pages \"level by level\":\n",
    "\n",
    "```\n",
    "              ðŸŒ Seed Page\n",
    "             /     |     \\\n",
    "          ðŸ“„ A    ðŸ“„ B    ðŸ“„ C      â† Level 1 (crawled first)\n",
    "          /  \\      |\n",
    "       ðŸ“„ D  ðŸ“„ E  ðŸ“„ F            â† Level 2 (crawled second)\n",
    "```\n",
    "\n",
    "**Why BFS?** Pages closer to the seed are usually more relevant. BFS finds them first before wandering off into unrelated topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl Wikipedia starting from \"Search engine\"\n",
    "# This takes about 30-40 seconds\n",
    "SEED_URL = \"https://en.wikipedia.org/wiki/Search_engine\"\n",
    "MAX_PAGES = 100\n",
    "\n",
    "crawled_data = crawl_wikipedia(SEED_URL, max_pages=MAX_PAGES)\n",
    "\n",
    "# Show what we crawled\n",
    "print(\"\\nPages crawled:\")\n",
    "for url, data in list(crawled_data.items())[:10]:\n",
    "    print(f\"  - {data['title']}\")\n",
    "print(f\"  ... and {len(crawled_data) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¾ Optional: Save crawled data to avoid re-crawling\n",
    "import json\n",
    "\n",
    "def save_crawled_data(data: dict, filename: str = \"crawled_data.json\") -> None:\n",
    "    \"\"\"Save crawled data to a JSON file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"âœ… Saved {len(data)} pages to {filename}\")\n",
    "\n",
    "# Uncomment to save:\n",
    "# save_crawled_data(crawled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‚ Optional: Load previously crawled data (skip crawling)\n",
    "import json\n",
    "\n",
    "def load_crawled_data(filename: str = \"crawled_data.json\") -> dict:\n",
    "    \"\"\"Load crawled data from a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"âœ… Loaded {len(data)} pages from {filename}\")\n",
    "    return data\n",
    "\n",
    "# Uncomment to load instead of crawling:\n",
    "# crawled_data = load_crawled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Link Graph\n",
    "\n",
    "Let's see how our crawled pages are connected! We'll build a **link graph** where:\n",
    "- **Nodes** = Wikipedia pages\n",
    "- **Edges** = Links between pages\n",
    "\n",
    "This is exactly how search engines model the web. You can **double-click any node** to open that Wikipedia page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import os\n",
    "\n",
    "def build_link_graph(crawled_data: dict) -> \"nx.DiGraph\":\n",
    "    \"\"\"Build a directed graph from crawled data.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    crawled_urls = set(crawled_data.keys())\n",
    "    \n",
    "    # Add nodes\n",
    "    for url, data in crawled_data.items():\n",
    "        G.add_node(url, title=data['title'])\n",
    "    \n",
    "    # Add edges (only between crawled pages)\n",
    "    for url, data in crawled_data.items():\n",
    "        for link in data['links']:\n",
    "            if link in crawled_urls:\n",
    "                G.add_edge(url, link)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_link_graph(G: \"nx.DiGraph\", crawled_data: dict, filename: str = \"link_graph.html\") -> str:\n",
    "    \"\"\"Create an interactive visualization - double-click nodes to open URLs!\"\"\"\n",
    "    net = Network(height=\"500px\", width=\"100%\", directed=True, notebook=False)\n",
    "    net.barnes_hut(gravity=-3000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Calculate in-degrees for node sizing\n",
    "    in_degrees = dict(G.in_degree())\n",
    "    max_deg = max(in_degrees.values()) if in_degrees else 1\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        title = G.nodes[node].get('title', node)\n",
    "        size = 10 + (in_degrees.get(node, 0) / max_deg) * 30\n",
    "        hover = f\"{title}\\n{in_degrees.get(node, 0)} backlinks\\nDouble-click to open\"\n",
    "        net.add_node(node, label=title[:20], title=hover, size=size)\n",
    "    \n",
    "    for source, target in G.edges():\n",
    "        net.add_edge(source, target)\n",
    "    \n",
    "    filepath = os.path.abspath(filename)\n",
    "    net.save_graph(filepath)\n",
    "    \n",
    "    # Add double-click handler to open URLs\n",
    "    with open(filepath, 'r') as f:\n",
    "        html = f.read()\n",
    "    \n",
    "    click_script = \"\"\"\n",
    "    <script>\n",
    "    network.on(\"doubleClick\", function(params) {\n",
    "        if (params.nodes.length > 0) {\n",
    "            var nodeId = params.nodes[0];\n",
    "            window.open(nodeId, '_blank');\n",
    "        }\n",
    "    });\n",
    "    </script>\n",
    "    </body>\n",
    "    \"\"\"\n",
    "    html = html.replace(\"</body>\", click_script)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Build and visualize the link graph\n",
    "link_graph = build_link_graph(crawled_data)\n",
    "print(f\"Graph: {link_graph.number_of_nodes()} pages, {link_graph.number_of_edges()} links\")\n",
    "\n",
    "graph_file = visualize_link_graph(link_graph, crawled_data)\n",
    "print(f\"\\nGraph saved to: {graph_file}\")\n",
    "print(\"Open this file in your browser, or it will appear in the Gradio app later!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Custom Documents\n",
    "\n",
    "Besides Wikipedia pages, you can add your own documents to the corpus:\n",
    "- **Text documents** - Paste or type content directly\n",
    "- **PDF files** - Extract text from PDF documents\n",
    "\n",
    "This is useful for:\n",
    "- Testing search with your own notes or study materials\n",
    "- Adding domain-specific content not found on Wikipedia\n",
    "- Experimenting with how different document types affect search quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Our document store - combines crawled pages with custom documents\n",
    "documents = {}\n",
    "\n",
    "# Add crawled pages to documents\n",
    "for url, data in crawled_data.items():\n",
    "    documents[url] = {\n",
    "        'title': data['title'],\n",
    "        'text': data['text'],\n",
    "        'source': 'wikipedia'\n",
    "    }\n",
    "\n",
    "def add_custom_document(title: str, text: str, source: str = \"custom\", url: str | None = None) -> str:\n",
    "    \"\"\"Add a custom text document to our corpus.\"\"\"\n",
    "    doc_id = f\"custom_{len([k for k in documents if k.startswith('custom_')])}\" \n",
    "    documents[doc_id] = {\n",
    "        'title': title,\n",
    "        'text': text,\n",
    "        'source': source,\n",
    "        'url': url\n",
    "    }\n",
    "    print(f\"Added document: {title}\")\n",
    "    return doc_id\n",
    "\n",
    "def process_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}\"\n",
    "\n",
    "def add_pdf_document(file_path: str, title: str | None = None) -> str:\n",
    "    \"\"\"Add a PDF document to our corpus.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PDF file\n",
    "        title: Optional title (defaults to filename if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        The document ID, or error message if extraction failed\n",
    "    \"\"\"\n",
    "    text = process_pdf(file_path)\n",
    "    if text.startswith(\"Error\"):\n",
    "        print(text)\n",
    "        return text\n",
    "    \n",
    "    # Use filename as title if not provided\n",
    "    if title is None:\n",
    "        import os\n",
    "        title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    doc_id = add_custom_document(title, text, source=\"pdf\", url=file_path)\n",
    "    print(f\"  Extracted {len(text):,} characters from PDF\")\n",
    "    return doc_id\n",
    "\n",
    "# Example 1: Add custom text documents\n",
    "add_custom_document(\n",
    "    \"GDP Growth in the Lion City\",\n",
    "    \"The Lion City has seen remarkable economic expansion in recent years. Financial services and technology sectors have driven significant growth in gross domestic product. The city-state continues to attract global investment.\"\n",
    ")\n",
    "\n",
    "add_custom_document(\n",
    "    \"How Automobiles Changed Transportation\", \n",
    "    \"The motor vehicle revolutionized how people travel. Machines powered by internal combustion engines replaced horse-drawn carriages. Modern automobiles feature advanced safety systems and increasingly electric powertrains.\"\n",
    ")\n",
    "\n",
    "# Example 2: Add a PDF document about BM25\n",
    "# This PDF wasn't in our Wikipedia crawl, so it adds new knowledge to our corpus!\n",
    "add_pdf_document(\"bm25_intro.pdf\", \"BM25 Algorithm Introduction\")\n",
    "\n",
    "print(f\"\\nTotal documents in corpus: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Lexical Search with BM25\n",
    "\n",
    "### What is Lexical (Keyword) Search?\n",
    "\n",
    "Lexical search finds documents by matching exact words. The key concepts are:\n",
    "\n",
    "**Term Frequency (TF):** How often does the search term appear in a document?\n",
    "- \"apple\" appears 5 times â†’ higher score\n",
    "\n",
    "**Inverse Document Frequency (IDF):** How rare is the term across all documents?\n",
    "- \"the\" appears everywhere â†’ low IDF (not useful for ranking)\n",
    "- \"BM25\" is rare â†’ high IDF (very useful for ranking)\n",
    "\n",
    "**Document Length Normalization:** Longer documents naturally contain more words, so we adjust for length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 - \"Best Match 25\"\n",
    "\n",
    "BM25 is the evolution of TF-IDF, and is the algorithm behind:\n",
    "- **Elasticsearch** (default ranking)\n",
    "- **Apache Lucene/Solr**\n",
    "- **Many production search systems**\n",
    "\n",
    "The formula (simplified):\n",
    "```\n",
    "score = IDF * (TF * (k1 + 1)) / (TF + k1 * (1 - b + b * docLength/avgDocLength))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `k1` controls term frequency saturation (typically 1.2-2.0)\n",
    "- `b` controls document length normalization (typically 0.75)\n",
    "\n",
    "Don't worry about the math - the `rank_bm25` library handles this for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Simple tokenization: lowercase and split on non-alphanumeric.\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Prepare documents for BM25\n",
    "doc_ids = list(documents.keys())\n",
    "doc_texts = [documents[doc_id]['text'] for doc_id in doc_ids]\n",
    "doc_titles = [documents[doc_id]['title'] for doc_id in doc_ids]\n",
    "\n",
    "# Tokenize all documents\n",
    "tokenized_docs = [tokenize(text) for text in doc_texts]\n",
    "\n",
    "# Build BM25 index\n",
    "bm25_index = BM25Okapi(tokenized_docs)\n",
    "\n",
    "print(f\"BM25 index built with {len(doc_ids)} documents!\")\n",
    "print(f\"Average document length: {sum(len(d) for d in tokenized_docs) / len(tokenized_docs):.0f} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query: str, top_k: int = 5) -> list[tuple[str, float]]:\n",
    "    \"\"\"Search using BM25 and return top_k results.\"\"\"\n",
    "    tokenized_query = tokenize(query)\n",
    "    scores = bm25_index.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top k results\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'doc_id': doc_ids[idx],\n",
    "            'title': doc_titles[idx],\n",
    "            'score': scores[idx],\n",
    "            'text_preview': doc_texts[idx][:200] + '...'\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BM25 search\n",
    "test_queries = [\"search engine\", \"web crawler\", \"information retrieval\", \"bm25\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    results = bm25_search(query, top_k=3)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. {r['title']} (score: {r['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Strengths & Weaknesses\n",
    "\n",
    "**Strengths:**\n",
    "- Exact keyword matches rank highly\n",
    "- Rare/specific terms get boosted (great for technical searches)\n",
    "- Fast - no GPU or complex models needed\n",
    "- Interpretable - you know exactly WHY a document matched\n",
    "\n",
    "**Weaknesses:**\n",
    "- \"Singapore economy\" won't find \"Lion City GDP\"\n",
    "- Typos break searches (\"serch engne\" â†’ no results)\n",
    "- No understanding of synonyms (\"car\" won't match \"automobile\")\n",
    "- Word order doesn't matter (\"dog bites man\" = \"man bites dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Where BM25 fails\n",
    "print(\"=== BM25 Failure Cases ===\")\n",
    "\n",
    "# Semantic mismatch - no keyword overlap\n",
    "print(\"\\n1. Semantic mismatch: 'Singapore economy'\")\n",
    "print(\"   Looking for our custom doc 'GDP Growth in the Lion City'...\")\n",
    "results = bm25_search(\"Singapore economy\", top_k=3)\n",
    "found = any(\"Lion City\" in r['title'] for r in results)\n",
    "if found:\n",
    "    print(\"   Found it!\")\n",
    "else:\n",
    "    print(\"   NOT FOUND - BM25 misses it because 'Singapore' and 'economy' aren't in the doc!\")\n",
    "print(\"   Top results instead:\", [r['title'][:30] for r in results])\n",
    "\n",
    "# Synonyms - \"car\" vs \"automobile\"\n",
    "print(\"\\n2. Synonyms: 'car vehicle' (looking for 'Automobiles' doc)\")\n",
    "results = bm25_search(\"car vehicle\", top_k=3)\n",
    "found = any(\"Automobile\" in r['title'] for r in results)\n",
    "if found:\n",
    "    print(\"   Found it!\")\n",
    "else:\n",
    "    print(\"   NOT FOUND - BM25 doesn't know 'car' = 'automobile'!\")\n",
    "print(\"   Top results instead:\", [r['title'][:30] for r in results])\n",
    "\n",
    "# Paraphrase - different words, same meaning\n",
    "print(\"\\n3. Paraphrase: 'city-state financial growth' (looking for 'Lion City GDP' doc)\")\n",
    "results = bm25_search(\"city-state financial growth\", top_k=3)\n",
    "found = any(\"Lion City\" in r['title'] for r in results)\n",
    "if found:\n",
    "    print(\"   Found it!\")\n",
    "else:\n",
    "    print(\"   NOT FOUND - BM25 needs exact keywords!\")\n",
    "print(\"   Top results:\", [r['title'][:30] for r in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Where BM25 excels\n",
    "print(\"=== BM25 Success Cases ===\")\n",
    "\n",
    "# Our BM25 PDF document\n",
    "print(\"\\n1. Exact technical term: 'BM25 term frequency saturation'\")\n",
    "print(\"   (This exact phrase is in our bm25_intro.pdf)\")\n",
    "results = bm25_search(\"BM25 term frequency saturation\", top_k=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    marker = \" <-- From our PDF!\" if \"BM25\" in r['title'] else \"\"\n",
    "    print(f\"   {i}. {r['title']} (score: {r['score']:.2f}){marker}\")\n",
    "\n",
    "# Specific rare terms\n",
    "print(\"\\n2. Rare technical term: 'PageRank algorithm'\")\n",
    "results = bm25_search(\"PageRank algorithm\", top_k=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"   {i}. {r['title']} (score: {r['score']:.2f})\")\n",
    "\n",
    "# Named entities\n",
    "print(\"\\n3. Specific product name: 'Elasticsearch'\")\n",
    "results = bm25_search(\"Elasticsearch\", top_k=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"   {i}. {r['title']} (score: {r['score']:.2f})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ BM25 excels when you know the exact keywords in the documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Semantic Search with Embeddings\n",
    "\n",
    "### What is Semantic Search?\n",
    "\n",
    "Semantic search finds documents based on **meaning**, not just keywords.\n",
    "\n",
    "The key insight: We can convert text into **vectors** (lists of numbers) where:\n",
    "- Similar meanings â†’ similar vectors\n",
    "- Different meanings â†’ different vectors\n",
    "\n",
    "This is done using **neural network models** trained on massive amounts of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Embeddings Work\n",
    "\n",
    "```\n",
    "\"Search engine\"     â†’ [0.12, -0.45, 0.78, ..., 0.33]  (384 numbers)\n",
    "\"Information retrieval\" â†’ [0.14, -0.42, 0.75, ..., 0.31]  (similar!)\n",
    "\"Chocolate cake\"    â†’ [-0.67, 0.23, -0.11, ..., -0.89]  (very different)\n",
    "```\n",
    "\n",
    "**Measuring similarity:**\n",
    "- **Cosine similarity:** angle between vectors (1 = identical, 0 = unrelated)\n",
    "- **L2 distance (Euclidean):** straight-line distance (0 = identical)\n",
    "\n",
    "We use **FAISS** (Facebook AI Similarity Search) to efficiently find similar vectors among millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load embedding model\n",
    "# all-MiniLM-L6-v2 is small (80MB) and fast, but still effective\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Test it\n",
    "test_sentences = [\"Search engine\", \"Information retrieval\", \"Chocolate cake\"]\n",
    "test_embeddings = embed_model.encode(test_sentences)\n",
    "\n",
    "print(f\"\\nEmbedding dimension: {test_embeddings.shape[1]}\")\n",
    "print(f\"Sample embedding (first 10 values): {test_embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index for semantic search\n",
    "print(\"Encoding all documents...\")\n",
    "\n",
    "# Encode all document texts\n",
    "doc_embeddings = embed_model.encode(doc_texts, show_progress_bar=True)\n",
    "doc_embeddings = np.array(doc_embeddings).astype('float32')\n",
    "\n",
    "# Build FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "faiss_index.add(doc_embeddings)\n",
    "\n",
    "print(f\"\\nFAISS index built with {faiss_index.ntotal} vectors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 5) -> list[tuple[str, float]]:\n",
    "    \"\"\"Search using semantic similarity.\"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embed_model.encode([query]).astype('float32')\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        # Convert L2 distance to similarity score (higher = more similar)\n",
    "        similarity = 1 / (1 + distances[0][i])\n",
    "        results.append({\n",
    "            'doc_id': doc_ids[idx],\n",
    "            'title': doc_titles[idx],\n",
    "            'score': similarity,\n",
    "            'text_preview': doc_texts[idx][:200] + '...'\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "test_queries = [\"finding information on the internet\", \"how websites get ranked\", \"storing and retrieving data\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    results = semantic_search(query, top_k=3)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. {r['title']} (similarity: {r['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search Strengths & Weaknesses\n",
    "\n",
    "**Strengths:**\n",
    "- Understands synonyms (\"car\" finds \"automobile\")\n",
    "- Handles paraphrasing (\"how to find info\" matches \"information retrieval\")\n",
    "- Cross-lingual search possible with multilingual models\n",
    "- Works well for conceptual/exploratory queries\n",
    "\n",
    "**Weaknesses:**\n",
    "- May miss exact keyword matches (looking for \"BM25\" might return general search articles)\n",
    "- Less interpretable (hard to explain WHY something matched)\n",
    "- Requires embedding model (more compute, ~100-500ms per query)\n",
    "- Can struggle with rare proper nouns and technical terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Where Semantic Search shines\n",
    "print(\"=== Semantic Search Success Cases ===\")\n",
    "\n",
    "# Same queries that BM25 failed - semantic succeeds!\n",
    "print(\"\\n1. Finding 'Lion City' doc with 'Singapore economy':\")\n",
    "print(\"   (Remember: BM25 failed because 'Singapore'/'economy' aren't in the doc)\")\n",
    "results = semantic_search(\"Singapore economy\", top_k=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    marker = \" <-- FOUND IT!\" if \"Lion City\" in r['title'] else \"\"\n",
    "    print(f\"   {i}. {r['title']} (similarity: {r['score']:.3f}){marker}\")\n",
    "\n",
    "# Synonyms\n",
    "print(\"\\n2. Finding 'Automobiles' doc with 'car vehicle':\")\n",
    "print(\"   (BM25 failed because 'car'/'vehicle' aren't in the doc - it uses 'motor vehicle', 'automobile')\")\n",
    "results = semantic_search(\"car vehicle\", top_k=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    marker = \" <-- FOUND IT!\" if \"Automobile\" in r['title'] else \"\"\n",
    "    print(f\"   {i}. {r['title']} (similarity: {r['score']:.3f}){marker}\")\n",
    "\n",
    "# Intent/meaning based\n",
    "print(\"\\n3. Intent-based: 'how websites get discovered and indexed'\")\n",
    "results = semantic_search(\"how websites get discovered and indexed\", top_k=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"   {i}. {r['title']} (similarity: {r['score']:.3f})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Semantic search understands meaning, synonyms, and intent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Where Semantic Search can struggle\n",
    "print(\"=== Semantic Search Challenges ===\")\n",
    "\n",
    "# Very specific technical strings\n",
    "print(\"\\n1. Very specific phrase: 'k1 parameter 1.2 to 2.0'\")\n",
    "print(\"   (This exact text is in our BM25 PDF)\")\n",
    "bm25_results = bm25_search(\"k1 parameter 1.2 to 2.0\", top_k=1)\n",
    "sem_results = semantic_search(\"k1 parameter 1.2 to 2.0\", top_k=1)\n",
    "print(f\"   BM25 top result: {bm25_results[0]['title']}\")\n",
    "print(f\"   Semantic top result: {sem_results[0]['title']}\")\n",
    "\n",
    "# Code/technical jargon\n",
    "print(\"\\n2. Technical acronym: 'TF-IDF'\")\n",
    "bm25_results = bm25_search(\"TF-IDF\", top_k=3)\n",
    "sem_results = semantic_search(\"TF-IDF\", top_k=3)\n",
    "print(f\"   BM25: {[r['title'][:25] for r in bm25_results]}\")\n",
    "print(f\"   Semantic: {[r['title'][:25] for r in sem_results]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ For exact technical terms and specific strings, BM25 is often more precise!\")\n",
    "print(\"   This is why hybrid search combines both approaches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Hybrid Search - Best of Both Worlds\n",
    "\n",
    "### Why Hybrid?\n",
    "\n",
    "Neither BM25 nor semantic search is perfect:\n",
    "\n",
    "| Query Type | BM25 | Semantic | Winner |\n",
    "|------------|------|----------|--------|\n",
    "| Exact term: \"BM25 algorithm\" | Great | OK | BM25 |\n",
    "| Conceptual: \"finding stuff online\" | Poor | Great | Semantic |\n",
    "| Mixed: \"PageRank for SEO\" | Good | Good | Tie |\n",
    "\n",
    "**Hybrid search combines both** to get:\n",
    "- Exact keyword matching when needed\n",
    "- Semantic understanding for conceptual queries\n",
    "\n",
    "**Who uses hybrid search?**\n",
    "- Elasticsearch (8.0+)\n",
    "- Vespa\n",
    "- Weaviate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF is simple but effective. The idea:\n",
    "1. Run both BM25 and semantic search\n",
    "2. For each document, calculate: `score = 1 / (k + rank)`\n",
    "3. Sum scores from both methods\n",
    "4. Re-rank by combined score\n",
    "\n",
    "The `k` parameter (typically 60) controls how much we trust each ranking.\n",
    "\n",
    "**Why RRF works:**\n",
    "- Documents that rank high in BOTH methods get boosted\n",
    "- No need to normalize different score scales\n",
    "- No hyperparameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(bm25_results: list, semantic_results: list, k: int = 60) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Combine BM25 and semantic results using Reciprocal Rank Fusion.\n",
    "    \n",
    "    Args:\n",
    "        bm25_results: List of results from BM25 search\n",
    "        semantic_results: List of results from semantic search\n",
    "        k: RRF constant (typically 60)\n",
    "    \n",
    "    Returns:\n",
    "        Combined results sorted by RRF score\n",
    "    \"\"\"\n",
    "    rrf_scores = {}\n",
    "    doc_info = {}  # Store document info for results\n",
    "    \n",
    "    # Add BM25 contributions\n",
    "    for rank, result in enumerate(bm25_results):\n",
    "        doc_id = result['doc_id']\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k + rank + 1)\n",
    "        doc_info[doc_id] = result\n",
    "    \n",
    "    # Add semantic contributions\n",
    "    for rank, result in enumerate(semantic_results):\n",
    "        doc_id = result['doc_id']\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k + rank + 1)\n",
    "        if doc_id not in doc_info:\n",
    "            doc_info[doc_id] = result\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for doc_id, score in sorted_docs:\n",
    "        results.append({\n",
    "            'doc_id': doc_id,\n",
    "            'title': doc_info[doc_id]['title'],\n",
    "            'score': score,\n",
    "            'text_preview': doc_info[doc_id]['text_preview']\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Weighted Score Combination\n",
    "\n",
    "Another approach is to directly combine scores:\n",
    "\n",
    "```\n",
    "final_score = alpha * normalize(bm25_score) + (1 - alpha) * normalize(semantic_score)\n",
    "```\n",
    "\n",
    "Where `alpha` controls the balance:\n",
    "- `alpha = 1.0` â†’ pure BM25\n",
    "- `alpha = 0.0` â†’ pure semantic\n",
    "- `alpha = 0.5` â†’ equal weight\n",
    "\n",
    "**Challenge:** BM25 and semantic scores are on different scales, so we need to normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(results: list[tuple[str, float]]) -> dict[str, float]:\n",
    "    \"\"\"Normalize scores to 0-1 range using min-max normalization.\"\"\"\n",
    "    if not results:\n",
    "        return results\n",
    "    \n",
    "    scores = [r['score'] for r in results]\n",
    "    min_score, max_score = min(scores), max(scores)\n",
    "    \n",
    "    if max_score == min_score:\n",
    "        # All scores are the same\n",
    "        for r in results:\n",
    "            r['normalized_score'] = 1.0\n",
    "    else:\n",
    "        for r in results:\n",
    "            r['normalized_score'] = (r['score'] - min_score) / (max_score - min_score)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def weighted_hybrid_search(query: str, alpha: float = 0.5, top_k: int = 10) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Combine BM25 and semantic search with weighted scores.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        alpha: Weight for BM25 (1-alpha for semantic)\n",
    "        top_k: Number of results\n",
    "    \n",
    "    Returns:\n",
    "        Combined results sorted by weighted score\n",
    "    \"\"\"\n",
    "    # Get results from both methods\n",
    "    bm25_results = normalize_scores(bm25_search(query, top_k=top_k))\n",
    "    semantic_results = normalize_scores(semantic_search(query, top_k=top_k))\n",
    "    \n",
    "    # Build score dictionaries\n",
    "    bm25_scores = {r['doc_id']: r['normalized_score'] for r in bm25_results}\n",
    "    semantic_scores = {r['doc_id']: r['normalized_score'] for r in semantic_results}\n",
    "    doc_info = {r['doc_id']: r for r in bm25_results + semantic_results}\n",
    "    \n",
    "    # Get all unique doc_ids\n",
    "    all_docs = set(bm25_scores.keys()) | set(semantic_scores.keys())\n",
    "    \n",
    "    # Calculate weighted scores\n",
    "    combined_scores = {}\n",
    "    for doc_id in all_docs:\n",
    "        bm25_s = bm25_scores.get(doc_id, 0)\n",
    "        semantic_s = semantic_scores.get(doc_id, 0)\n",
    "        combined_scores[doc_id] = alpha * bm25_s + (1 - alpha) * semantic_s\n",
    "    \n",
    "    # Sort and return\n",
    "    sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for doc_id, score in sorted_docs:\n",
    "        results.append({\n",
    "            'doc_id': doc_id,\n",
    "            'title': doc_info[doc_id]['title'],\n",
    "            'score': score,\n",
    "            'text_preview': doc_info[doc_id]['text_preview']\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, method: str = 'rrf', alpha: float = 0.5, top_k: int = 5) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Unified hybrid search function.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        method: 'rrf' for Reciprocal Rank Fusion, 'weighted' for weighted combination\n",
    "        alpha: Weight for BM25 (only used with 'weighted' method)\n",
    "        top_k: Number of results\n",
    "    \"\"\"\n",
    "    if method == 'rrf':\n",
    "        bm25_results = bm25_search(query, top_k=top_k*2)  # Get more for fusion\n",
    "        semantic_results = semantic_search(query, top_k=top_k*2)\n",
    "        return reciprocal_rank_fusion(bm25_results, semantic_results)[:top_k]\n",
    "    else:\n",
    "        return weighted_hybrid_search(query, alpha=alpha, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three approaches\n",
    "def compare_search_methods(query: str, comment: str = \"\") -> None:\n",
    "    \"\"\"Compare BM25, Semantic, and Hybrid search results.\"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    if comment:\n",
    "        print(f\"  ({comment})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    bm25_results = bm25_search(query, top_k=3)\n",
    "    semantic_results = semantic_search(query, top_k=3)\n",
    "    hybrid_results = hybrid_search(query, method='rrf', top_k=3)\n",
    "    \n",
    "    print(f\"{'Rank':<5} {'BM25':<25} {'Semantic':<25} {'Hybrid (RRF)':<25}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i in range(3):\n",
    "        bm25_title = bm25_results[i]['title'][:22] + \"..\" if len(bm25_results[i]['title']) > 22 else bm25_results[i]['title']\n",
    "        sem_title = semantic_results[i]['title'][:22] + \"..\" if len(semantic_results[i]['title']) > 22 else semantic_results[i]['title']\n",
    "        hyb_title = hybrid_results[i]['title'][:22] + \"..\" if len(hybrid_results[i]['title']) > 22 else hybrid_results[i]['title']\n",
    "        print(f\"{i+1:<5} {bm25_title:<25} {sem_title:<25} {hyb_title:<25}\")\n",
    "\n",
    "# Test queries that clearly show different behaviors\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARING SEARCH METHODS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Query 1: Both work well (baseline)\n",
    "compare_search_methods(\"search engine\", \"Both methods should work - exact keywords + clear intent\")\n",
    "\n",
    "# Query 2: Semantic wins - no keyword overlap\n",
    "compare_search_methods(\"Singapore economy\", \"Semantic wins: 'Lion City GDP' doc has no 'Singapore'/'economy' keywords\")\n",
    "\n",
    "# Query 3: BM25 wins - exact technical terms from our PDF\n",
    "compare_search_methods(\"BM25 term frequency\", \"BM25 wins: exact technical phrase from our PDF\")\n",
    "\n",
    "# Query 4: Semantic wins - synonyms/paraphrase  \n",
    "compare_search_methods(\"car vehicle travel\", \"Semantic wins: 'Automobiles' doc uses different words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CASE STUDY: Why Hybrid Search Wins\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Example where each method has partial success\n",
    "query = \"Singapore economy growth\"\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"\\nWe have a document 'GDP Growth in the Lion City' about Singapore's economy.\")\n",
    "print(\"Challenge: It doesn't contain 'Singapore' or 'economy' as keywords!\\n\")\n",
    "\n",
    "bm25_results = bm25_search(query, top_k=5)\n",
    "semantic_results = semantic_search(query, top_k=5)\n",
    "hybrid_results = hybrid_search(query, method='rrf', top_k=5)\n",
    "\n",
    "# Check where Lion City appears in each\n",
    "def find_position(results, keyword):\n",
    "    for i, r in enumerate(results):\n",
    "        if keyword in r['title']:\n",
    "            return i + 1\n",
    "    return None\n",
    "\n",
    "lion_bm25 = find_position(bm25_results, \"Lion City\")\n",
    "lion_sem = find_position(semantic_results, \"Lion City\")  \n",
    "lion_hyb = find_position(hybrid_results, \"Lion City\")\n",
    "\n",
    "print(\"BM25 results:\")\n",
    "for i, r in enumerate(bm25_results[:3], 1):\n",
    "    marker = \" <-- Target doc!\" if \"Lion City\" in r['title'] else \"\"\n",
    "    print(f\"  {i}. {r['title']}{marker}\")\n",
    "print(f\"  â†’ Lion City doc position: {lion_bm25 if lion_bm25 else 'Not in top 5'}\")\n",
    "\n",
    "print(\"\\nSemantic results:\")\n",
    "for i, r in enumerate(semantic_results[:3], 1):\n",
    "    marker = \" <-- Target doc!\" if \"Lion City\" in r['title'] else \"\"\n",
    "    print(f\"  {i}. {r['title']}{marker}\")\n",
    "print(f\"  â†’ Lion City doc position: {lion_sem if lion_sem else 'Not in top 5'}\")\n",
    "\n",
    "print(\"\\nHybrid (RRF) results:\")\n",
    "for i, r in enumerate(hybrid_results[:3], 1):\n",
    "    marker = \" <-- Target doc!\" if \"Lion City\" in r['title'] else \"\"\n",
    "    print(f\"  {i}. {r['title']}{marker}\")\n",
    "print(f\"  â†’ Lion City doc position: {lion_hyb if lion_hyb else 'Not in top 5'}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"   - BM25 fails completely (no keyword match)\")\n",
    "print(\"   - Semantic understands 'Singapore' = 'Lion City' conceptually\")\n",
    "print(\"   - Hybrid benefits from semantic's understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: When to Use What - Decision Framework\n",
    "\n",
    "### Quick Decision Guide\n",
    "\n",
    "| Use Case | Recommended | Why |\n",
    "|----------|-------------|-----|\n",
    "| Exact product SKU/ID lookup | **BM25** | Need exact matches |\n",
    "| \"Find similar articles\" | **Semantic** | Conceptual similarity |\n",
    "| General content search | **Hybrid** | Best of both worlds |\n",
    "| Low latency required (<10ms) | **BM25** | No embedding computation |\n",
    "| Multilingual content | **Semantic** | Models understand multiple languages |\n",
    "| Technical documentation | **BM25** | Exact term matching important |\n",
    "| E-commerce product search | **Hybrid** | Users use various terms |\n",
    "| Legal/medical search | **Hybrid** | Precision + understanding |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Examples\n",
    "\n",
    "**Google Search:**\n",
    "- Uses hybrid approach: BERT for semantic understanding + traditional signals (links, keywords)\n",
    "- Introduced \"neural matching\" in 2018, then BERT in 2019\n",
    "\n",
    "**Elasticsearch:**\n",
    "- Default: BM25\n",
    "- 8.0+: Added kNN search for semantic/vector search\n",
    "- Hybrid via `_rank_feature` queries\n",
    "\n",
    "- Sophisticated hybrid search across billions of pages\n",
    "- Keyword filters + relevance scoring\n",
    "- Finds content \"about\" topics even with different wording\n",
    "\n",
    "**ChatGPT Retrieval (RAG):**\n",
    "- Primarily semantic search on embeddings\n",
    "- Some implementations add BM25 for keyword grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How hybrid search powers it:**\n",
    "\n",
    "1. **Keyword Matching (BM25-like):**\n",
    "   - Exact phrase matches\n",
    "   - \"In title\" filter\n",
    "   - Specific word requirements\n",
    "\n",
    "2. **Relevance Scoring (Semantic-like):**\n",
    "   - Finds content \"about\" a topic\n",
    "   - Understands related concepts\n",
    "   - Ranks by topical relevance\n",
    "\n",
    "3. **Authority Signals (Beyond search):**\n",
    "   - Domain Rating\n",
    "   - Traffic estimates\n",
    "   - Social shares\n",
    "\n",
    "**Why SEO professionals need both:**\n",
    "- Find exact competitor articles (BM25 - specific keywords)\n",
    "- Discover related content opportunities (Semantic - topical exploration)\n",
    "- Content gap analysis requires understanding BOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive comparison with different alpha values\n",
    "print(\"Effect of alpha on weighted hybrid search\")\n",
    "print(\"(alpha=1.0 is pure BM25, alpha=0.0 is pure Semantic)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"finding information online\"\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "\n",
    "for alpha in [1.0, 0.7, 0.5, 0.3, 0.0]:\n",
    "    results = hybrid_search(query, method='weighted', alpha=alpha, top_k=3)\n",
    "    print(f\"alpha={alpha}: {results[0]['title'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Document Chunking for RAG\n",
    "\n",
    "Now let's build something practical: a **Smart Flashcard Generator**!\n",
    "\n",
    "To do this, we need to understand **RAG (Retrieval-Augmented Generation)**:\n",
    "1. **Retrieve** relevant content using our hybrid search\n",
    "2. **Augment** the LLM prompt with this context\n",
    "3. **Generate** flashcards using an LLM\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "Before we can use our documents with an LLM, we need to **chunk** them into smaller pieces:\n",
    "\n",
    "- **LLMs have context limits** - we can't send entire Wikipedia pages\n",
    "- **Smaller chunks = more precise retrieval** - find the exact relevant paragraph\n",
    "- **Overlapping chunks** prevent losing context at chunk boundaries\n",
    "\n",
    "Think of it like this: Instead of searching for \"which book has info about X\", we search for \"which paragraph explains X\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: There are more sophisticated chunking methods (e.g., LangChain's RecursiveCharacterTextSplitter,\n",
    "# semantic chunking, etc.) but we'll keep it simple with sentence-based chunking.\n",
    "\n",
    "def chunk_text(text: str, target_size: int = 1000, overlap_sentences: int = 2) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks at sentence boundaries.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        target_size: Target size of each chunk (in characters)\n",
    "        overlap_sentences: Number of sentences to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (handles . ! ?)\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return [text] if text.strip() else []\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # If adding this sentence exceeds target and we have content, start new chunk\n",
    "        if current_size + len(sentence) > target_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            # Keep last N sentences for overlap (context continuity)\n",
    "            current_chunk = current_chunk[-overlap_sentences:] if overlap_sentences else []\n",
    "            current_size = sum(len(s) for s in current_chunk)\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_size += len(sentence)\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test the chunking function\n",
    "test_text = \"This is the first sentence. Here comes the second one! And what about the third? Finally, the fourth sentence arrives.\"\n",
    "test_chunks = chunk_text(test_text, target_size=60, overlap_sentences=1)\n",
    "print(\"Test chunks (with sentence boundaries):\")\n",
    "for i, chunk in enumerate(test_chunks):\n",
    "    print(f\"  Chunk {i+1}: '{chunk}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all documents and track metadata\n",
    "chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for doc_id, doc in documents.items():\n",
    "    doc_chunks = chunk_text(doc['text'], target_size=1000, overlap_sentences=2)\n",
    "    for i, chunk in enumerate(doc_chunks):\n",
    "        chunks.append(chunk)\n",
    "        chunk_metadata.append({\n",
    "            'doc_id': doc_id,\n",
    "            'title': doc['title'],\n",
    "            'chunk_idx': i,\n",
    "            'source': doc.get('source', 'unknown'),\n",
    "            'url': doc.get('url') or (doc_id if doc_id.startswith('http') else None)\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"Average chunk length: {sum(len(c) for c in chunks) / len(chunks):.0f} characters\")\n",
    "\n",
    "# Show example chunks from one document\n",
    "print(f\"\\nExample chunks from '{chunk_metadata[0]['title']}':\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"  Chunk {i+1}: {chunk[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild BM25 index on chunks\n",
    "tokenized_chunks = [tokenize(chunk) for chunk in chunks]\n",
    "chunk_bm25_index = BM25Okapi(tokenized_chunks)\n",
    "print(f\"BM25 index rebuilt with {len(chunks)} chunks\")\n",
    "\n",
    "# Rebuild FAISS index on chunks\n",
    "print(\"Encoding chunks for semantic search...\")\n",
    "chunk_embeddings = embed_model.encode(chunks, show_progress_bar=True)\n",
    "chunk_embeddings = np.array(chunk_embeddings).astype('float32')\n",
    "\n",
    "chunk_faiss_index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "chunk_faiss_index.add(chunk_embeddings)\n",
    "print(f\"FAISS index rebuilt with {chunk_faiss_index.ntotal} chunk vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search_chunks(query: str, top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"Search chunks using BM25.\"\"\"\n",
    "    tokenized_query = tokenize(query)\n",
    "    scores = chunk_bm25_index.get_scores(tokenized_query)\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk_idx': idx,\n",
    "            'text': chunks[idx],\n",
    "            'score': scores[idx],\n",
    "            'title': chunk_metadata[idx]['title'],\n",
    "            'doc_id': chunk_metadata[idx]['doc_id']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def semantic_search_chunks(query: str, top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"Search chunks using semantic similarity.\"\"\"\n",
    "    query_embedding = embed_model.encode([query]).astype('float32')\n",
    "    distances, indices = chunk_faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        similarity = 1 / (1 + distances[0][i])\n",
    "        results.append({\n",
    "            'chunk_idx': idx,\n",
    "            'text': chunks[idx],\n",
    "            'score': similarity,\n",
    "            'title': chunk_metadata[idx]['title'],\n",
    "            'doc_id': chunk_metadata[idx]['doc_id']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def hybrid_search_chunks(query: str, top_k: int = 5, k: int = 60) -> list[dict]:\n",
    "    \"\"\"Hybrid search on chunks using RRF.\"\"\"\n",
    "    bm25_results = bm25_search_chunks(query, top_k=top_k*2)\n",
    "    semantic_results = semantic_search_chunks(query, top_k=top_k*2)\n",
    "    \n",
    "    rrf_scores = {}\n",
    "    chunk_info = {}\n",
    "    \n",
    "    for rank, result in enumerate(bm25_results):\n",
    "        idx = result['chunk_idx']\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)\n",
    "        chunk_info[idx] = result\n",
    "    \n",
    "    for rank, result in enumerate(semantic_results):\n",
    "        idx = result['chunk_idx']\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)\n",
    "        if idx not in chunk_info:\n",
    "            chunk_info[idx] = result\n",
    "    \n",
    "    sorted_chunks = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx, score in sorted_chunks:\n",
    "        result = chunk_info[idx].copy()\n",
    "        result['score'] = score\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test chunk search\n",
    "print(\"Testing chunk-based hybrid search for 'search engine ranking':\")\n",
    "results = hybrid_search_chunks(\"search engine ranking\", top_k=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. From '{r['title']}':\")\n",
    "    print(f\"   {r['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: LLM-Powered Flashcard Generation\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** is a technique that combines:\n",
    "1. **Retrieval** - Find relevant context using search (our hybrid search!)\n",
    "2. **Augmentation** - Add this context to the LLM prompt\n",
    "3. **Generation** - LLM generates output using the context\n",
    "\n",
    "This is how tools like ChatGPT with browsing, Perplexity, and AI writing assistants work!\n",
    "\n",
    "### Our Model: Qwen3-8B\n",
    "\n",
    "We'll use **Qwen3-8B**, one of the best small LLMs available in 2025:\n",
    "- **High quality** - Beats much larger models on benchmarks\n",
    "- **Runs locally** - No API keys needed\n",
    "- **Works everywhere** - Colab (GPU) and Mac (Apple Silicon)\n",
    "\n",
    "**Memory optimization:**\n",
    "- On Colab: Uses 4-bit quantization (~4GB VRAM) - fits easily on free tier!\n",
    "- On Mac M4: Uses full precision with unified memory\n",
    "\n",
    "âš ï¸ **First run downloads the model (~8GB). This takes a few minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Detect environment\n",
    "def get_device_config() -> dict:\n",
    "    \"\"\"Detect the best device and configuration for the current environment.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Colab or NVIDIA GPU - use 4-bit quantization\n",
    "        print(\"ðŸ–¥ï¸  NVIDIA GPU detected - using 4-bit quantization\")\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        return {\"quantization_config\": bnb_config, \"device_map\": \"auto\"}\n",
    "    \n",
    "    elif torch.backends.mps.is_available():\n",
    "        # Mac Apple Silicon\n",
    "        print(\"ðŸŽ Apple Silicon detected - using MPS\")\n",
    "        return {\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\n",
    "    \n",
    "    else:\n",
    "        # CPU fallback\n",
    "        print(\"ðŸ’» CPU mode - this will be slow\")\n",
    "        return {\"device_map\": \"auto\", \"torch_dtype\": torch.float32}\n",
    "\n",
    "# Load the model\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "print(\"(First run downloads ~8GB - this takes a few minutes)\")\n",
    "print()\n",
    "\n",
    "device_config = get_device_config()\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    **device_config\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"âœ… Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_new_tokens: int = 200) -> str:\n",
    "    \"\"\"Generate text using Qwen3.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    text = llm_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # Disable thinking mode for faster responses\n",
    "    )\n",
    "    \n",
    "    inputs = llm_tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    return llm_tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing the model...\")\n",
    "response = generate_text(\"What is a search engine? Answer in one sentence.\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flashcard(topic: str, context: str, source_title: str | None = None, \n",
    "                       source_url: str | None = None, previous_questions: list[str] | None = None) -> dict:\n",
    "    \"\"\"Generate a study flashcard using Qwen3.\"\"\"\n",
    "    \n",
    "    # Build the prompt\n",
    "    avoid_section = \"\"\n",
    "    if previous_questions:\n",
    "        avoid_section = f\"\"\"\n",
    "IMPORTANT: Do NOT ask about these topics (already covered):\n",
    "{chr(10).join(f'- {q[:80]}' for q in previous_questions[-5:])}\n",
    "\n",
    "Ask about something DIFFERENT from the above.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Create a study flashcard about \"{topic}\" based on this context.\n",
    "\n",
    "Context: {context[:1500]}\n",
    "{avoid_section}\n",
    "Create ONE flashcard with:\n",
    "- A clear question that tests understanding of a key concept\n",
    "- A complete answer (1-3 sentences)\n",
    "\n",
    "Format your response EXACTLY like this:\n",
    "Question: [your question]\n",
    "Answer: [your answer]\"\"\"\n",
    "\n",
    "    response = generate_text(prompt, max_new_tokens=250)\n",
    "    \n",
    "    # Parse the response\n",
    "    question = \"\"\n",
    "    answer = \"\"\n",
    "    \n",
    "    # Look for Question: and Answer: markers\n",
    "    if \"Question:\" in response and \"Answer:\" in response:\n",
    "        parts = response.split(\"Answer:\")\n",
    "        question_part = parts[0]\n",
    "        answer_part = parts[1] if len(parts) > 1 else \"\"\n",
    "        \n",
    "        # Extract question\n",
    "        if \"Question:\" in question_part:\n",
    "            question = question_part.split(\"Question:\")[-1].strip()\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = answer_part.strip()\n",
    "        # Clean up: stop at double newlines or additional markers\n",
    "        for stop in [\"\\n\\n\", \"\\nQuestion:\", \"\\nNote:\", \"\\n---\"]:\n",
    "            if stop in answer:\n",
    "                answer = answer.split(stop)[0].strip()\n",
    "    \n",
    "    # Fallback parsing\n",
    "    if not question or not answer:\n",
    "        if \"?\" in response:\n",
    "            idx = response.rindex(\"?\")\n",
    "            question = response[:idx+1].strip()\n",
    "            answer = response[idx+1:].strip()\n",
    "    \n",
    "    # Build the card\n",
    "    card = {\n",
    "        'question': question if question else \"(Generation failed - please retry)\",\n",
    "        'answer': answer if answer else \"(No answer generated)\",\n",
    "        'topic': topic\n",
    "    }\n",
    "    \n",
    "    # Include source with URL if available\n",
    "    if source_url:\n",
    "        card['source'] = source_url  # Direct link\n",
    "        card['source_title'] = source_title\n",
    "    else:\n",
    "        card['source'] = source_title or 'Custom'\n",
    "        card['source_title'] = source_title\n",
    "    \n",
    "    return card\n",
    "\n",
    "# Test flashcard generation\n",
    "test_context = \"\"\"A search engine is a software system designed to carry out web searches. \n",
    "They search the World Wide Web in a systematic way for particular information specified in a textual web search query. \n",
    "The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs).\n",
    "Google is the most widely used search engine, processing over 8.5 billion searches per day.\"\"\"\n",
    "\n",
    "print(\"Generating test flashcard...\")\n",
    "card = generate_flashcard(\"search engines\", test_context, \"Wikipedia\", \"https://en.wikipedia.org/wiki/Search_engine\")\n",
    "print(f\"\\nâœ… Generated flashcard:\")\n",
    "print(f\"   Q: {card['question']}\")\n",
    "print(f\"   A: {card['answer']}\")\n",
    "print(f\"   Source: {card['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flashcards_for_topic(topic: str, num_cards: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Full RAG pipeline: Retrieve relevant chunks â†’ Generate flashcards.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to generate flashcards for\n",
    "        num_cards: Number of flashcards to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of flashcard dictionaries\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_cards} flashcards for '{topic}'...\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks using hybrid search\n",
    "    retrieved_chunks = hybrid_search_chunks(topic, top_k=num_cards * 3)\n",
    "    print(f\"  Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "    \n",
    "    # Step 2: Generate flashcard from each unique chunk\n",
    "    flashcards = []\n",
    "    seen_sources = set()  # Avoid duplicate sources\n",
    "    previous_questions = []  # Track questions to avoid duplicates\n",
    "    \n",
    "    for chunk in retrieved_chunks:\n",
    "        if len(flashcards) >= num_cards:\n",
    "            break\n",
    "        \n",
    "        # Skip if we already have a card from nearby chunks in same document\n",
    "        doc_key = (chunk['doc_id'], chunk.get('chunk_idx', 0) // 3)\n",
    "        if doc_key in seen_sources:\n",
    "            continue\n",
    "        seen_sources.add(doc_key)\n",
    "        \n",
    "        # Generate flashcard, passing previous questions for diversity\n",
    "        print(f\"  Generating card from '{chunk['title']}'...\")\n",
    "        card = generate_flashcard(\n",
    "            topic, \n",
    "            chunk['text'], \n",
    "            chunk['title'], \n",
    "            chunk.get('url'),\n",
    "            previous_questions=previous_questions\n",
    "        )\n",
    "        \n",
    "        # Track this question for diversity\n",
    "        if card['question'] and not card['question'].startswith(\"(\"):\n",
    "            previous_questions.append(card['question'])\n",
    "        \n",
    "        flashcards.append(card)\n",
    "    \n",
    "    print(f\"Generated {len(flashcards)} flashcards!\")\n",
    "    return flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full RAG pipeline!\n",
    "topic = \"web crawler\"\n",
    "flashcards = generate_flashcards_for_topic(topic, num_cards=3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"FLASHCARDS FOR: {topic}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, card in enumerate(flashcards, 1):\n",
    "    print(f\"\\nðŸ“ Card {i}\")\n",
    "    print(f\"   Q: {card['question']}\")\n",
    "    print(f\"   A: {card['answer']}\")\n",
    "    print(f\"   Source: {card.get('source_title', 'Unknown')}\")\n",
    "    if card.get('source', '').startswith('http'):\n",
    "        print(f\"   URL: {card['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Better Flashcards\n",
    "\n",
    "**Topic selection:**\n",
    "- More specific topics = better cards (\"PageRank algorithm\" vs \"search\")\n",
    "- Use the same terminology as your source documents\n",
    "\n",
    "**Quality depends on:**\n",
    "- Quality of source documents (garbage in = garbage out)\n",
    "- How well the hybrid search retrieves relevant content\n",
    "- The LLM's ability to extract key facts\n",
    "\n",
    "**After generation:**\n",
    "- Review and edit cards before studying\n",
    "- Remove duplicates or low-quality cards\n",
    "- Add your own cards for topics not well covered\n",
    "\n",
    "**Pro tip:** The flashcards work best when studying content you've already added to the corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flashcards_batch(topics: list[str], cards_per_topic: int = 3) -> list[dict]:\n",
    "    \"\"\"Generate flashcards for multiple topics.\"\"\"\n",
    "    all_cards = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        cards = generate_flashcards_for_topic(topic, num_cards=cards_per_topic)\n",
    "        all_cards.extend(cards)\n",
    "        print()  # Add spacing between topics\n",
    "    \n",
    "    return all_cards\n",
    "\n",
    "# Generate cards for multiple topics\n",
    "study_topics = [\"search engine\", \"web indexing\", \"Google\"]\n",
    "all_flashcards = generate_flashcards_batch(study_topics, cards_per_topic=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generated {len(all_flashcards)} total flashcards across {len(study_topics)} topics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Export to Anki & CSV\n",
    "\n",
    "Now let's make our flashcards actually useful! We'll export them to:\n",
    "\n",
    "1. **CSV** - Universal format, import into any flashcard app\n",
    "2. **Anki (.apkg)** - Direct import into [Anki](https://apps.ankiweb.net/), the most popular flashcard app\n",
    "\n",
    "### Why Anki?\n",
    "\n",
    "Anki uses **spaced repetition** - it shows you cards just before you'd forget them. This is scientifically proven to be the most efficient way to memorize information!\n",
    "\n",
    "- Free and open source\n",
    "- Available on all platforms (desktop, mobile, web)\n",
    "- Used by medical students, language learners, and programmers worldwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def export_to_csv(flashcards: list[dict], filename: str = \"flashcards.csv\") -> str:\n",
    "    \"\"\"\n",
    "    Export flashcards to CSV format.\n",
    "    \n",
    "    CSV can be imported into most flashcard apps including Anki, Quizlet, etc.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Question', 'Answer', 'Topic', 'Source', 'URL'])\n",
    "        \n",
    "        for card in flashcards:\n",
    "            writer.writerow([\n",
    "                card['question'],\n",
    "                card['answer'],\n",
    "                card.get('topic', ''),\n",
    "                card.get('source_title', card.get('source', '')),\n",
    "                card.get('source', '') if str(card.get('source', '')).startswith('http') else ''\n",
    "            ])\n",
    "    \n",
    "    print(f\"âœ… Exported {len(flashcards)} cards to {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Export our flashcards to CSV\n",
    "csv_file = export_to_csv(all_flashcards, \"study_flashcards.csv\")\n",
    "print(f\"\\nCSV file created! You can import this into Quizlet, Anki, or any flashcard app.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genanki\n",
    "import random\n",
    "\n",
    "# Define the Anki note model (card template)\n",
    "FLASHCARD_MODEL = genanki.Model(\n",
    "    random.randrange(1 << 30, 1 << 31),  # Unique model ID\n",
    "    'Simple Q&A',\n",
    "    fields=[\n",
    "        {'name': 'Question'},\n",
    "        {'name': 'Answer'},\n",
    "        {'name': 'Topic'},\n",
    "        {'name': 'Source'},\n",
    "    ],\n",
    "    templates=[\n",
    "        {\n",
    "            'name': 'Card 1',\n",
    "            'qfmt': '''\n",
    "                <div style=\"font-size: 20px; text-align: center;\">\n",
    "                    {{Question}}\n",
    "                </div>\n",
    "                <div style=\"font-size: 12px; color: #666; margin-top: 20px;\">\n",
    "                    Topic: {{Topic}}\n",
    "                </div>\n",
    "            ''',\n",
    "            'afmt': '''\n",
    "                {{FrontSide}}\n",
    "                <hr>\n",
    "                <div style=\"font-size: 18px; text-align: center;\">\n",
    "                    {{Answer}}\n",
    "                </div>\n",
    "                <div style=\"font-size: 10px; color: #999; margin-top: 20px;\">\n",
    "                    Source: {{Source}}\n",
    "                </div>\n",
    "            ''',\n",
    "        },\n",
    "    ],\n",
    "    css='''\n",
    "        .card {\n",
    "            font-family: Arial, sans-serif;\n",
    "            background-color: #fafafa;\n",
    "            padding: 20px;\n",
    "        }\n",
    "    '''\n",
    ")\n",
    "\n",
    "def export_to_anki(flashcards: list[dict], filename: str = \"flashcards.apkg\", deck_name: str = \"Study Deck\") -> str:\n",
    "    \"\"\"\n",
    "    Export flashcards to Anki format (.apkg).\n",
    "    \n",
    "    The .apkg file can be directly imported into Anki.\n",
    "    \"\"\"\n",
    "    # Create a new deck\n",
    "    deck = genanki.Deck(\n",
    "        random.randrange(1 << 30, 1 << 31),  # Unique deck ID\n",
    "        deck_name\n",
    "    )\n",
    "    \n",
    "    # Add each flashcard as a note\n",
    "    for card in flashcards:\n",
    "        note = genanki.Note(\n",
    "            model=FLASHCARD_MODEL,\n",
    "            fields=[\n",
    "                card['question'],\n",
    "                card['answer'],\n",
    "                card.get('topic', ''),\n",
    "                card.get('source', '')\n",
    "            ]\n",
    "        )\n",
    "        deck.add_note(note)\n",
    "    \n",
    "    # Create the package and save\n",
    "    package = genanki.Package(deck)\n",
    "    package.write_to_file(filename)\n",
    "    \n",
    "    print(f\"âœ… Exported {len(flashcards)} cards to {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Anki format\n",
    "anki_file = export_to_anki(all_flashcards, \"search_study_deck.apkg\", \"Search Engines Study Deck\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files created:\")\n",
    "print(f\"   - {csv_file} (for Quizlet, Google Sheets, etc.)\")\n",
    "print(f\"   - {anki_file} (for Anki app)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_pdf() and add_pdf_document() are defined in Part 2\n",
    "# Here we add a function to rebuild indices after adding new documents\n",
    "\n",
    "def add_notes_to_corpus(title: str, text: str) -> str:\n",
    "    \"\"\"Add user notes to the document corpus and rebuild indices.\"\"\"\n",
    "    global documents, chunks, chunk_metadata, chunk_bm25_index, chunk_embeddings, chunk_faiss_index\n",
    "    global doc_ids, doc_texts, doc_titles, tokenized_docs, bm25_index, doc_embeddings, faiss_index\n",
    "    \n",
    "    # Add to documents\n",
    "    doc_id = add_custom_document(title, text, source=\"user_notes\")\n",
    "    \n",
    "    # Rebuild document-level indices\n",
    "    doc_ids = list(documents.keys())\n",
    "    doc_texts = [documents[d]['text'] for d in doc_ids]\n",
    "    doc_titles = [documents[d]['title'] for d in doc_ids]\n",
    "    tokenized_docs = [tokenize(text) for text in doc_texts]\n",
    "    bm25_index = BM25Okapi(tokenized_docs)\n",
    "    doc_embeddings = embed_model.encode(doc_texts).astype('float32')\n",
    "    faiss_index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "    faiss_index.add(doc_embeddings)\n",
    "    \n",
    "    # Rebuild chunk-level indices\n",
    "    chunks = []\n",
    "    chunk_metadata = []\n",
    "    for doc_id, doc in documents.items():\n",
    "        doc_chunks = chunk_text(doc['text'], target_size=1000, overlap_sentences=2)\n",
    "        for i, chunk in enumerate(doc_chunks):\n",
    "            chunks.append(chunk)\n",
    "            chunk_metadata.append({\n",
    "                'doc_id': doc_id,\n",
    "                'title': doc['title'],\n",
    "                'chunk_idx': i,\n",
    "                'source': doc.get('source', 'unknown'),\n",
    "                'url': doc.get('url') or (doc_id if doc_id.startswith('http') else None)\n",
    "            })\n",
    "    \n",
    "    tokenized_chunks = [tokenize(chunk) for chunk in chunks]\n",
    "    chunk_bm25_index = BM25Okapi(tokenized_chunks)\n",
    "    chunk_embeddings = embed_model.encode(chunks).astype('float32')\n",
    "    chunk_faiss_index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "    chunk_faiss_index.add(chunk_embeddings)\n",
    "    \n",
    "    return f\"Added '{title}' ({len(text)} chars) to corpus. Now have {len(documents)} documents, {len(chunks)} chunks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# App state for generated flashcards\n",
    "generated_cards = []\n",
    "\n",
    "def generate_cards_ui(topics_text: str, num_cards: int) -> tuple[str, list]:\n",
    "    \"\"\"Generate flashcards from UI input.\"\"\"\n",
    "    global generated_cards\n",
    "    \n",
    "    if not topics_text.strip():\n",
    "        return \"Please enter at least one topic.\", []\n",
    "    \n",
    "    # Parse topics (comma or newline separated)\n",
    "    topics = [t.strip() for t in topics_text.replace('\\n', ',').split(',') if t.strip()]\n",
    "    \n",
    "    # Generate flashcards\n",
    "    generated_cards = generate_flashcards_batch(topics, cards_per_topic=int(num_cards))\n",
    "    \n",
    "    # Format for display\n",
    "    rows = []\n",
    "    for i, card in enumerate(generated_cards, 1):\n",
    "        rows.append([i, card['question'], card['answer'], card.get('topic', ''), card.get('source_title', 'Custom'), card.get('source', '') if card.get('source', '').startswith('http') else ''])\n",
    "    \n",
    "    return f\"Generated {len(generated_cards)} flashcards for {len(topics)} topic(s)!\", rows\n",
    "\n",
    "def export_csv_ui() -> str:\n",
    "    \"\"\"Export current flashcards to CSV.\"\"\"\n",
    "    if not generated_cards:\n",
    "        return \"No flashcards to export. Generate some first!\"\n",
    "    filename = export_to_csv(generated_cards, \"flashcards_export.csv\")\n",
    "    return f\"Exported to {filename}\"\n",
    "\n",
    "def export_anki_ui(deck_name: str) -> str:\n",
    "    \"\"\"Export current flashcards to Anki.\"\"\"\n",
    "    if not generated_cards:\n",
    "        return \"No flashcards to export. Generate some first!\"\n",
    "    filename = export_to_anki(generated_cards, \"flashcards_export.apkg\", deck_name or \"Study Deck\")\n",
    "    return f\"Exported to {filename}\"\n",
    "\n",
    "def upload_pdf_ui(file, title: str) -> tuple[str, str]:\n",
    "    \"\"\"Handle PDF upload.\"\"\"\n",
    "    if file is None:\n",
    "        return \"Please upload a PDF file.\", \"\"\n",
    "    \n",
    "    text = process_pdf(file.name)\n",
    "    if text.startswith(\"Error\"):\n",
    "        return text, \"\"\n",
    "    \n",
    "    return f\"Extracted {len(text)} characters from PDF.\", text[:2000] + (\"...\" if len(text) > 2000 else \"\")\n",
    "\n",
    "def upload_text_ui(title: str, text: str) -> str:\n",
    "    \"\"\"Add text notes to corpus.\"\"\"\n",
    "    if not title.strip() or not text.strip():\n",
    "        return \"Please provide both title and text.\"\n",
    "    \n",
    "    result = add_notes_to_corpus(title, text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete Gradio app\n",
    "with gr.Blocks(title=\"Smart Flashcard Generator\", theme=gr.themes.Soft()) as flashcard_app:\n",
    "    gr.Markdown(\"# ðŸŽ“ Smart Flashcard Generator\")\n",
    "    gr.Markdown(\"Generate study flashcards from your notes using AI + Hybrid Search\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Tab 1: Generate Flashcards\n",
    "        with gr.Tab(\"ðŸ“ Generate Flashcards\"):\n",
    "            gr.Markdown(\"### Generate Flashcards for Topics\")\n",
    "            gr.Markdown(\"Enter topics (comma or newline separated) to generate flashcards from the document corpus.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    topics_input = gr.Textbox(\n",
    "                        label=\"Topics to Study\",\n",
    "                        placeholder=\"search engine, web crawler, PageRank\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                with gr.Column(scale=1):\n",
    "                    num_cards_slider = gr.Slider(\n",
    "                        minimum=1, maximum=5, value=3, step=1,\n",
    "                        label=\"Cards per Topic\"\n",
    "                    )\n",
    "            \n",
    "            generate_btn = gr.Button(\"ðŸš€ Generate Flashcards\", variant=\"primary\")\n",
    "            gen_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            \n",
    "            flashcards_table = gr.Dataframe(\n",
    "                headers=[\"#\", \"Question\", \"Answer\", \"Topic\", \"Source\", \"URL\"],\n",
    "                interactive=False,\n",
    "                wrap=True\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### Export\")\n",
    "            with gr.Row():\n",
    "                export_csv_btn = gr.Button(\"ðŸ“„ Export to CSV\")\n",
    "                deck_name_input = gr.Textbox(label=\"Anki Deck Name\", value=\"Study Deck\", scale=2)\n",
    "                export_anki_btn = gr.Button(\"ðŸ“š Export to Anki\")\n",
    "            export_status = gr.Textbox(label=\"Export Status\", interactive=False)\n",
    "            \n",
    "            generate_btn.click(\n",
    "                fn=generate_cards_ui,\n",
    "                inputs=[topics_input, num_cards_slider],\n",
    "                outputs=[gen_status, flashcards_table]\n",
    "            )\n",
    "            export_csv_btn.click(fn=export_csv_ui, outputs=export_status)\n",
    "            export_anki_btn.click(fn=export_anki_ui, inputs=deck_name_input, outputs=export_status)\n",
    "        \n",
    "        # Tab 2: Upload Notes\n",
    "        with gr.Tab(\"ðŸ“¤ Upload Notes\"):\n",
    "            gr.Markdown(\"### Add Your Own Notes\")\n",
    "            gr.Markdown(\"Upload a PDF or paste text to add to the searchable corpus.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"**Upload PDF**\")\n",
    "                    pdf_file = gr.File(label=\"Select PDF\", file_types=[\".pdf\"])\n",
    "                    pdf_title = gr.Textbox(label=\"Document Title\", placeholder=\"My Study Notes\")\n",
    "                    pdf_upload_btn = gr.Button(\"Process PDF\")\n",
    "                    pdf_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                    pdf_preview = gr.Textbox(label=\"Preview\", lines=10, interactive=False)\n",
    "                \n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"**Or Paste Text**\")\n",
    "                    text_title = gr.Textbox(label=\"Document Title\", placeholder=\"Lecture Notes - Week 1\")\n",
    "                    text_content = gr.Textbox(label=\"Text Content\", lines=10, placeholder=\"Paste your notes here...\")\n",
    "                    text_add_btn = gr.Button(\"Add to Corpus\")\n",
    "                    text_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            \n",
    "            pdf_upload_btn.click(\n",
    "                fn=upload_pdf_ui,\n",
    "                inputs=[pdf_file, pdf_title],\n",
    "                outputs=[pdf_status, pdf_preview]\n",
    "            )\n",
    "            text_add_btn.click(\n",
    "                fn=upload_text_ui,\n",
    "                inputs=[text_title, text_content],\n",
    "                outputs=text_status\n",
    "            )\n",
    "        \n",
    "        # Tab 3: Search Comparison (kept from before)\n",
    "        with gr.Tab(\"ðŸ” Compare Search\"):\n",
    "            gr.Markdown(\"### Compare BM25 vs Semantic vs Hybrid\")\n",
    "            compare_query = gr.Textbox(label=\"Search Query\", placeholder=\"Try: 'Singapore economy'\")\n",
    "            compare_btn = gr.Button(\"Compare Methods\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"**BM25**\")\n",
    "                    bm25_results = gr.Dataframe(headers=[\"Rank\", \"Title\", \"Score\"], interactive=False)\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"**Semantic**\")\n",
    "                    semantic_results = gr.Dataframe(headers=[\"Rank\", \"Title\", \"Score\"], interactive=False)\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"**Hybrid**\")\n",
    "                    hybrid_results = gr.Dataframe(headers=[\"Rank\", \"Title\", \"Score\"], interactive=False)\n",
    "            \n",
    "            compare_btn.click(\n",
    "                fn=compare_all_methods,\n",
    "                inputs=compare_query,\n",
    "                outputs=[bm25_results, semantic_results, hybrid_results]\n",
    "            )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    ### How This Works\n",
    "    1. **Upload** your notes (PDF or text) or use the pre-loaded Wikipedia pages\n",
    "    2. **Enter topics** you want to study\n",
    "    3. **Hybrid Search** finds relevant content (BM25 + Semantic)\n",
    "    4. **LLM** generates flashcards from the retrieved context\n",
    "    5. **Export** to Anki or CSV for spaced repetition study!\n",
    "    \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the complete Flashcard Generator app!\n",
    "flashcard_app.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Further Reading\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **BM25 (Lexical Search)**\n",
    "   - Matches exact keywords using TF-IDF principles\n",
    "   - Fast, interpretable, great for specific terms\n",
    "   - Fails on synonyms and conceptual queries\n",
    "\n",
    "2. **Semantic Search**\n",
    "   - Uses neural embeddings to understand meaning\n",
    "   - Handles synonyms, paraphrasing, conceptual queries\n",
    "   - Can miss exact keyword matches\n",
    "\n",
    "3. **Hybrid Search**\n",
    "   - Combines both approaches for best results\n",
    "   - RRF: Simple, no tuning needed\n",
    "   - Weighted: Adjustable balance via alpha parameter\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [BM25: The Next Generation of Lucene Relevance](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables)\n",
    "- [Sentence-BERT Paper](https://arxiv.org/abs/1908.10084)\n",
    "- [Reciprocal Rank Fusion Paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)\n",
    "- [Elasticsearch: Combining BM25 and kNN](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
